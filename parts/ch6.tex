\chapter{Implementing the procedure as a proof of concept}%
\label{ch:implementing}


In the preceding chapter, we explored how to translate definitional specifications of (co)datatypes to the fundamental constructions that can encode (co)datatypes as QPFs.
In the current chapter, we will discuss the technical details of implementing this procedure in Lean 4, as \data{}
and \codata{} commands.

To that end, we're going to dig deeper into parts of the meta-programming system. Nonetheless, we
will focus on only the parts that are relevant to our implementation, for a more comprehensive introduction
to meta-programming in Lean 4, the interested reader is invited to consult~\cite{paulinoMetaprogrammingLean, ullrichNotationsHygienicMacro2022}.


We still won't assume knowledge of Lean specifics, but do require some more familiarity with functional programming (roughly, readers should be somewhat familiar with monads, and how they are used for side effects, as explained in, e.g.,~\cite{christiansenFunctionalProgrammingLean}).


\section{Extending Lean's syntax}%
\label{sec:syntax}

Like many modern theorem provers, Lean has facilities that allow us to register custom syntax for specific functions. For example, the following macro allows us to write \lean{Γ ⊢ 0 : Nat} in the familiar syntax for some type checking relation \lean{Typing Γ 0 Nat} we might define.
\begin{leancode}
    macro Γ:term " ⊢ " e:term " : " t:term : term => `(Typing $Γ $e $t)
\end{leancode}
Actually, this \keyword{macro} declaration is just syntactic sugar for the following two parts.

\begin{leancode}
  syntax term " ⊢ " term " : " term : term
  macro_rules
    | `($Γ ⊢ $e : $t) => `(Typing $Γ $e $t)
\end{leancode}

Firstly, \keyword{syntax} defines a \emph{parser extension}.
Lean's grammar does not have a clearly delimited macro invocation syntax, instead,
macros can freely extend Lean's syntax, influencing how the parser transforms source code into an \emph{abstract syntax tree} (AST).
The result of parsing, this AST, is represented in Lean as an object of type \lean{Syntax}.

\begin{remark}
    In more recent versions of Lean, the \lean{Syntax} type has been replaced by \lean{TSyntax cat}, where \lean{cat} is the (statically known) syntax category (e.g., \lean{term} or \lean{command}).
    Our code was written before this overhaul, thus we generally refer to the old way in this chapter.
\end{remark}


In this example, the parser extension states that three terms (bound to \lean{Γ}, \lean{e} and \lean{t}), interspersed with \lean{⊢} and \lean{:} symbols form a new term.

Then, the \keyword{macro\_rules}
part defines the \emph{semantics} of the macro, as a substitution. We both match on, and create new syntax trees, using \emph{syntax quotations} (written as \lean{`()}). We can use a variable of type \lean{Syntax} in a syntax quotation as, e.g, \lean{`(\$Γ)}, this is called an \emph{anti-quotation}. By substituting on the level of syntax trees, grouping and precedence are preserved. For example, \lean{Γ ⊢ 1 + 2 * 3 : Nat} is correctly translated
to \lean{Typing Γ (1 + 2 * 3) Nat}.

Clearly, though, \data{} and \codata{} would be hard to define in terms of such substitutions.
We could, in theory, replace the right-hand side of a \keyword{macro\_rules} match arm 
with arbitrary code that produces a \lean{Syntax} object, and in this way, define a \emph{procedural macro}.
This is, however, not idiomatic; complex commands are usually defined as an \emph{elaborator}.


\subsection*{Data and codata Syntax}%
\label{subsec:impl:syntax}

Before we get to what an elaborator is, we'll take a few steps back and consider how to define the parser extensions for \data{} and \codata{}.

The flexibility and extensibility of Lean are not just for users. The Lean 4 compiler is mostly written in Lean 4 itself, and it turns out that a lot of the language syntax is implemented with the meta-programming system. Amongst them, \inductive{},
whose syntax is defined with the following parser extension.
\begin{leancode}
  def «inductive» := leading_parser "inductive " >> declId >> optDeclSig 
                      >> optional (symbol " :=" <|> " where") 
                      >> many ctor 
                      >> optDeriving
\end{leancode}

\begin{remark}
  Notice how we wrote \lean{«...»} around \inductive{}; these brackets allow us to use 
  reserved keywords as identifiers.
\end{remark}

Where \lean{leading\_parser} is 
a still lower-level macro to define a parser extension than the \keyword{syntax} declaration we saw before, and
\lean{declId}, \lean{optDeclSig}, etc.\ are all parsers, or parser combinators. Combinators \lean{a >> b} and \lean{a <|> b} mean ``first parse \lean{a}, then \lean{b}'' and ``parse \lean{a} \emph{or} \lean{b}'', respectively.

This parser accepts an \lean{"inductive"} \emph{atom} (or, literal), followed by a declaration identifier (\lean{declId}), an optional signature (\lean{optDeclSig}), optionally a \lean{" :="} or \lean{"where"} atom, followed by zero or more (\lean{many}) constructor specifications (\lean{ctor}), and finally, an optional list of typeclasses to derive (\lean{optDeriving}).

This \lean{Parser} instance is then used in the \lean{declaration} parser.
\begin{leancode}
  @[builtinCommandParser] def declaration := leading_parser
          declModifiers false >> (/- ... -/ <|> «instance» <|> /- ... -/)
\end{leancode}

The \lean{builtinCommandParser} attribute registers \lean{declaration} as a builtin syntax extension for the \lean{command} category.

\begin{remark}
    There are many syntax categories, of which \lean{term} and \lean{command} are by far the most common.
    The former, \lean{term}, is the syntax category for terms, e.g., \lean{x + y} or \lean{\Type{} → Nat → \Type{}}, whereas \lean{command} is the category for top-level commands, such as \inductive{}, but also definitions (\keyword{def}), theorems (\keyword{theorem}), etc.
  \end{remark}

We want \data{} to be (mostly) suitable as a drop-in replacement for \inductive{}, so we copy the latter's parser verbatim, replacing the \lean{"inductive"} atom with \lean{"data"}. Similarly, \codata{} also uses the same syntax, except the command is now \lean{"codata"}.
\begin{leancode}
  def inductive_like (cmd : String) : Parser
    := leading_parser cmd >> declId  >> optDeclSig  
                        >> Parser.optional  (symbol " :=" <|> " where") 
                        >> many ctor 
                        >> optDeriving

  def data   := inductive_like "data "
  def codata := inductive_like "codata "
\end{leancode}

\begin{remark}
    The syntax highlighting is slightly misleading. At this point the parser does not know about \data{} and \codata{} as commands yet, so they don't need to be enclosed with \lean{«...»} brackets to be used as identifiers.
\end{remark}

Since we are not defining builtin syntax, we use the \lean{commandParser} attribute to define the parser extension
rather than \lean{builtinCommandParser}.

\begin{leancode}
  @[commandParser] def declaration := leading_parser 
      declModifiers false >> (data <|> codata)
\end{leancode}


After this definition, Lean knows how to successfully parse datatype specifications such as
\begin{badleancode}
  data Foo α
    | foo : α → Foo α
\end{badleancode}

However, we only defined the syntax, not the semantics. 
The code above still fails, complaining that we did not define an \emph{elaboration function} for \lean{declaration}.


\subsection*{Unsupported syntax}%
\label{subsec:impl:unsupported_syntax}

Note that parser extensions are not strictly about which syntax is \emph{supported}.
Instead, they declare which syntax this command is \emph{responsible} for.
Suppose, for example, that a user tried to derive a typeclass for their datatype.
\begin{badleancode}
  data Foo α
    | foo : α → Foo α
    deriving BEq
\end{badleancode}

Although this syntax is accepted by the \data{} parser, we don't actually support deriving typeclasses yet.

Removing the \lean{>> optDeriving} part from the parser definition would bring the accepted syntax closer to what is actually supported, making \lean{Foo} as written above lead to a generic parse error.

Nevertheless, the user is clearly trying to invoke our \data{} command, and \inductive{} does support such a \keyword{deriving} statement, so it is to be expected that users will try to use it. The generic parse errors are quite obscure and bad at communicating to the user that \keyword{deriving} statements are not supported; a user might reasonably think the parse error is because of some (non-existent) typo.

So instead, we have the parser accept exactly the same syntax as \inductive{}, and in the \emph{elaborator} we then throw a custom, informative error when a user tries to use unsupported constructs.
In this way, \data{} is truly a drop-in replacement for \inductive{}, syntax-wise, and any errors will guide the user and explain unsupported syntax.



\section{Defining the semantics of a command}
With the parser extension in place, the next step is to give our commands their semantics, by defining a corresponding \emph{command elaborator}.
That is, we have to define the Lean code that is run when a \data{} or \codata{} command is invoked.

We already mentioned that the two main syntax categories are terms and commands.
Both have a slightly different kind of elaborator.

Elaborating a term means to translate it to an expression of the core logic of Lean (as encoded by the \lean{Expr} type). A \emph{term elaborator} is thus a function \lean{Syntax → TermElabM Expr}, where \lean{TermElabM} is the term elaboration monad. This monad gives (read-only) access to the \emph{environment} (i.e., declarations and imports that the term can refer to) and \emph{metavariable context} (roughly speaking, a metavariable represents a hole in the program that should be synthesized/inferred at some point during compilation).


Commands, on the other hand, are not translated but used for their side effect. Hence, the type of a \emph{command elaborator} is \lean{Syntax → CommandElabM Unit}, where the \lean{CommandElabM} monad allows for four kinds of side effects: 
\begin{enumerate}
    \item Modifying the environment (e.g., \inductive{}, \keyword{def}, \keyword{theorem}),
    \item Logging messages to inform the user (e.g., \lean{\#check}, \lean{\#print}, \lean{\#eval}),
    \item Performing IO, and
    \item Throwing errors
\end{enumerate}

Although \lean{TermElabM} neither extends nor is extended by \lean{CommandElabM}, it is common to elaborate terms as part of a command elaborator, e.g., by using \lean{liftTermElabM} or \lean{runTermElabM}.


\subsection*{Elaborator registration}
Besides defining a function of the appropriate type, an elaborator also needs to be decorated with a \lean{commandElab} attribute.

\begin{leancode}
  @[commandElab declaration] def elabData : Syntax → CommandElabM Unit
\end{leancode}

The argument to the attribute, \lean{declaration}, refers to the parser we defined in the preceding section, which is how Lean knows to use this elaborator for \data{} and \codata{} commands.
Before going further into the implementation of \lean{elabData}, we'll go over some high-level choices 
that were made, and their trade-offs.


\section{Elaborating inductive declarations}
For the elaborator, too, we draw inspiration from how \inductive{} is implemented.
Elaboration starts in the generic declaration elaborator, which checks what kind of declaration it is given, and defers to \lean{elabInductive} (in case of an inductive declaration). The latter, in turn, calls \lean{inductiveSyntaxToView} to transform the \lean{Syntax} object into a \lean{InductiveView} and defers to \lean{elabInductiveViews}.

This \lean{InductiveView} is a thin wrapper around the syntax tree, allowing us to refer to the different parts of an inductive declaration by name, rather than offset, but crucially, still stores, e.g., constructor types as \lean{Syntax} objects.

At this point, auto-bound implicit variables are added, the constructor type terms are elaborated, and type universes are inferred (if needed), to produce an elaborated \lean{InductiveType} object. 
Finally, the \lean{InductiveType} is wrapped in a \lean{Declaration}, added to the environment, and auxiliary constructions are generated with \lean{mkAuxConstructions}.


Most of this work, besides the steps that modify the environment, is also relevant for \data{} and \codata{} declarations.

An attempt was made to copy the code of \lean{elabInductiveViews} (and all private or protected functions it called) and factor out all work that is relevant for both \inductive{} and \data{}/\codata{} declarations into a common function.
However, it turned out that some elaboration steps needed to produce the \lean{InductiveType} object were not desirable for then running the procedure of \cref{ch:procedure} on.

For example, type parameters are added to constructor types as implicit arguments during the elaboration steps.
\begin{leancode}
    inductive Foo α β
      | mk : α → β → Foo α β

    inductive Bar α
      | mk : {β : Type} → α → β → Bar α

    -- Foo.mk has type `{α: Type} → {β: Type} → α → β → Foo α β`
    -- Bar.mk has type `{α: Type} → {β: Type} → α → β → Bar α`
\end{leancode}
However, we would like to give as input to the procedure just (expressions corresponding to) types \lean{α → β → Foo α β} and \lean{{β : Type} → α → β → Bar α}.
It is, of course, possible to detect which of the implicit argument are type parameters, by looking at which variables are passed to \lean{Foo}, resp. \lean{Bar}, but this adds complexity.

In the end, the required changes were deemed to require too much engineering effort. 
Since the implementation is intended purely as a prototype, a simpler approach was taken:
most steps of the procedure are performed on the level of syntax, rather than on elaborated expressions. 
The composition pipeline is an exception: for that step we do have to elaborate the provided terms.


Concretely, a \lean{DataView} structure was defined as a thin wrapper around a \data{} or \codata{} syntax tree, just like \lean{InductiveView}.
This structure is then passed around to the different steps of the procedure, to represent the specification.



\section{Adding declarations to the environment}
A key part of \lean{elabData} is to generate declarations and add them to the environment.
This can be done in multiple ways, in roughly increasing order of both robustness and complexity:
\begin{enumerate}
    \item Generate source code as a \lean{String}
    \item Generate a \lean{Syntax} or \lean{InductiveView} object
    \item Generate a fully elaborated \lean{Declaration} object
\end{enumerate}

The first approach, while easy, is too brittle. There exists a function, \lean{runFrontend}, that can be used to parse and elaborate the code in a string, but it is designed to be called with the content of a \texttt{.lean} file of user-written source code, not with code generated during command elaboration.

The last approach requires full elaboration of all terms involved, which, as discussed in the previous section, was deemed too complicated for our prototype.

Therefore, the second approach was chosen, relying on \emph{syntax quotations} to simplify the generation of syntax trees.
Since \lean{InductiveView} is just a thin wrapper around syntax trees, we consider it as being part of the syntactic approach.

For example, if we want to define \lean{Foo} as the string \lean{"Bar"}, this could be done with
\begin{leancode}
    let stx : Syntax ← `(def Foo := "Bar")
    elabCommand stx
\end{leancode}
The variable \lean{stx} stores the parsed syntax tree, and \lean{elabCommand} is a builtin function \lean{Syntax → CommandElabM Unit} that takes a \lean{command} syntax tree and calls the elaborator that was registered for that particular kind of command.

By default, a syntax quotation will try to parse whatever code we write as either a command or a term. There are situations, however, where we want to build up a command (or term) incrementally, requiring us to generate syntax that, by itself, is not a complete command (or term). Imagine, for example, that, given some array \lean{[`A, `B, `C]} of names, we want to generate the following inductive type:
\begin{leancode}
    inductive Baz
      | A : Baz
      | B : Baz
      | C : Baz
\end{leancode}
The code to do so might look as follows:
\begin{leancode}
    open Lean.Parser.Command in
    def inductiveFromNames (names : Array Name) : CommandElabM Unit := do
      let Baz := mkIdent `Baz
      let ctors ← names.mapM fun name =>
      let ctorIdent := mkIdent name
        `(ctor| 
          | $ctorIdent:ident : $Baz:ident
        )

      elabCommand (←`(
        inductive $id:ident
          $[$ctors:ctor]*
      ))
\end{leancode}
Of special interest is the \lean{`(ctor| ...)} syntax quotation, which specifies that the syntax should be parsed with the \lean{ctor} parser. 

The shape of the syntax tree produced by a quotation is determined statically, not at runtime, so the parser cannot examine the values of the \lean{ctorIdent} and \lean{ident} anti-quotations, it treats them as opaque blobs of syntax. We have to help it a bit by specifying the syntax category explicitly, as \lean{:ident}.
Similarly, in the final \inductive{} syntax quotation, we specify that \lean{ctors} is an array of syntax trees of the \lean{ctor} category.

\begin{remark}
    With the overhaul we mentioned at the start of this section, syntax categories are now tracked in the 
    type system, making such hints obsolete in newer versions of Lean.
\end{remark}

The \lean{\$[...]*} part is called a \emph{splice}, and it allows us to use an array of syntax objects whenever a parser expects a whitespace-separated list of things (as is the case with the \lean{many} parser combinator used in the \inductive{} parser).

Take note that we have to create the identifier \lean{Baz} with the \lean{mkIdent} function if
we want it to be accessible. By default, Lean's macros are \emph{hygienic} (see~\cite{ullrichNotationsHygienicMacro2022}),
meaning that an identifier \lean{Baz} written directly into a syntax quotation is not accessible outside the macro, making sure they don't accidentally clash or shadow other identifiers at the call-site.
The \lean{mkIdent} function allows us to opt out of hygiene so that the identifier is just \lean{Baz} as intended.



% \section{Composition Pipeline}%
% \label{sec:impl:composition_pipeline}



\section{Implementing the elaborator}%
\label{sec:implementing}

Now that we have a general idea of the meta-programming system of Lean, and a high-level implementation strategy, let us 
briefly examine how \lean{elabData}, the elaborator for \data{} and \codata{} is implemented.
By design, the steps of the procedure we described in \cref{sec:procedure:overview} closely correspond to the different functions that make up the \lean{elabData} implementation.
We will now describe roughly the same overview, but with a focus
on which functions implement which step.

First, \lean{makeNonRecursive} takes a \lean{DataView} and returns a new, non-recursive \lean{DataView} (step~\ref{enum:proc:make_nonrec}).
Then, \lean{mkShape} takes this non-recursive view, defines \lean{Shape} as an inductive type (step~\ref{enum:proc:mkShape}) and calls \lean{mkQpf} to derive the corresponding polynomial functor and establish its isomorphism with \lean{Shape} (step~\ref{enum:proc:mkQpf}).

Continuing, \lean{elabQpfComposition} is the entry point into the composition pipeline (step~\ref{enum:proc:mkBase}), and is used to obtain a syntax tree that represents the \lean{Base} functor.
Nothing is added to the environment at this step.

Depending on which command was called, \lean{mkType} will take either the fixpoint or cofixpoint of \lean{Base}, and
add the result to the environment (step~\ref{enum:proc:fixpoint}). It will do so first in the uncurried form, as \lean{F.Uncurried}, and
then define \lean{F} as \lean{@TypeFun.curried n F.Uncurried}, where \lean{F} is whatever identifier the
user supplied and $n$ is the arity.
Finally, \lean{mkConstructors} will derive the expected constructor functions (step~\ref{enum:proc:aux})


% \begin{leancode}
%   @[commandElab declaration]
%   def elabData : CommandElab := fun stx => do 
%     let modifiers ← elabModifiers stx[0]
%     let decl := stx[1]
%     let view ← dataSyntaxToView modifiers decl
  
%     let (nonRecView, rho) ← makeNonRecursive view;
  
%     let ⟨r, shape, P⟩ ← mkShape nonRecView.asInductive
  
%     /- Composition pipeline -/
%     let base ← elabQpfCompositionBody {
%       liveBinders := nonRecView.liveBinders, 
%       deadBinders := nonRecView.deadBinders,     
%       type?   := none,
%       target  := ←`(
%         $(mkIdent shape):ident $r.expr*
%       )
%     }
%     trace[Qpf.Data] "base = {base}"
  
%     mkType view base  
%     mkConstructors view shape
% \end{leancode}

% We won't do any analysis of which steps may be skipped, instead accepting that we might generate less optimal constructions.

% The starting point is the elaborator itself, which is registered as such with the @[commandElab declaration]
% attribute. Here, \lean{declaration} refers to the parser extension we defined in \cref{subsec:impl:syntax},
% and tells Lean this elaborator can be used when syntax is parsed according to that particular parser.

% \begin{leancode}
%     @[commandElab declaration] def elabData : Syntax → CommandElabM Unit
% \end{leancode}

% We won't include implementation code in minute detail, merely discussing the execution flow at a high level.
% The implementation functions were organized around the declarations they add to the environment, which makes the callgraph 
% resemble the procedure overview in reverse order.

% After performing some sanity checks on the input (e.g., see \cref{subsec:impl:unsupported_syntax}), \lean{elabData}
% calls \lean{mkBase}, which is adds the \lean{Base} functor to the environment, and then defines the resulting type in terms of either the fixpoint or cofixpoint (step~\ref{enum:impl:fixpoint}), depending on whether the syntax its elaborating used the \data{} or \codata{} command.

% \begin{leancode}
%   /-- The "base" type is the shape type with all variables set to the appropriate
%       expressions, besides the variable used for (co)-recursive occurences.
%       It is the final step before taking the (co)fixpoint -/
%   def mkBase (view : InductiveView) : CommandElabM Syntax
% \end{leancode}

% First, \lean{mkBase} will defer to \lean{mkShape} to generate the \lean{Shape} type, including the
% corresponding instance of \lean{MvQpf}, whence it invokes the composition pipeline through \lean{elabQpfComposition}


% The relevant function signatures are
% \begin{leancode}
% -- In `Qpf.Macro.Data`

%   /-- Defines the "head" type of a polynomial functor-/
%   def mkHeadT (view : InductiveView) : CommandElabM Name

%   /-- Defines the "child" family of type vectors for an `n`-ary polynomial functor -/
%   def mkChildT (view : InductiveView) (r : Replace) (headTName : Name)
%                : CommandElabM Name

%   /-- Show that the `Shape` type is a qpf, through an isomorphism with the 
%       `Shape.P` pfunctor -/
%   def mkQpf (shapeView : InductiveView) (ctorArgs : Array CtorArgs) 
%             (headT childT P : Syntax) (arity : Nat) 
%             : CommandElabM Unit    


%   structure MkShapeResult := (r : Replace) (shape : Name) (P : Name)
%   /-- Define the "shape" polynomial functor -/
%   def mkShape (view: InductiveView) : CommandElabM MkShapeResult


%   /-- The "base" type is the shape type with all variables set to the appropriate
%       expressions, besides the variable used for (co)-recursive occurences.
%       It is the final step before taking the (co)fixpoint -/
%   def mkBase (view : InductiveView) : CommandElabM Syntax


%   /-- Top-level elaboration for both `data` and `codata` declarations -/
%   @[commandElab «declaration»] def elabData : CommandElab

% -- In `Qpf.Macro.Comp`
% \end{leancode}


% \begin{todo}
%     Finish this section
% \end{todo}
